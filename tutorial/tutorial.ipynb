{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPIbfYcy1dCU"
   },
   "source": [
    "# **GaNDLF Tutorial: Classification Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFAtdeZ3w1EV"
   },
   "source": [
    "In this tutorial, we will be using the Generally Nuanced Deep Learning Framework (GaNDLF) to perform training and inference on a VGG model with PathMNIST, a dataset of colon pathology images. This is a multi-class classification task: there are 9 different types of colon tissue displayed in the pathology images, each represented by its own class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aIKvdPcJxUgx"
   },
   "source": [
    "This tutorial demonstrates how to use GaNDLF with a simple classification task. Some steps that would ordinarily be part of the workflow (e.g. data CSV and config YAML file construction) have already been performed for simplicity; please refer to the GaNDLF documentation (located at https://mlcommons.github.io/GaNDLF) for more information regarding replication of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXcwo7tixb7z"
   },
   "source": [
    "**Important**: Before continuing with this tutorial, please ensure that you are connected to the GPU by navigating to **Runtime --> Change Runtime Type --> Hardware Accelerator** and verifying that \"GPU\" is listed as the selected option. If not, it is highly recommended that you switch to it now. Also, if available, select the \"High-RAM\" option under **Runtime --> Change Runtime Type --> Runtime Shape**. Without this option selected, you may end up running out of RAM during training on a base notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPWyAwB8y09B"
   },
   "source": [
    "## Lets Get Started\n",
    "\n",
    "### Importing Data\n",
    "\n",
    "Let's import MedMNIST data and verify the version number before we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdzGc-3DzANz",
    "outputId": "697adfc4-7000-43ac-cd10-3aa96cf3c3e8",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl7_x7WHy91p"
   },
   "source": [
    "#### Time to load our data! Let's download all MedMNIST datasets to the root directory. In this tutorial, we will only be using the PathMNIST dataset; however, feel free to use any of the other datasets you see being downloaded below to try out GaNDLF for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qMX6gcLwzlu3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "99569f15-60d9-4f71-b146-6cdfc9e6356f",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!python -m medmnist download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKtgI5vjzHA7"
   },
   "source": [
    "#### Now, let's save all PathMNIST pathology images within the dataset folder (located inside the medmnist directory) in PNG format for use in training and inference. \n",
    "\n",
    "#### If you've already gone through this tutorial and are looking to try using a different MedMNIST dataset, simply change `--flag=pathmnist` to any of the other datasets that were downloaded aboveâ€”it's as simple as that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-3_mqgC8zIJU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "baa5ba76-91e7-47ae-b37e-7b70e7e07a47",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!python -m medmnist save --flag=pathmnist --folder=medmnist/dataset/ --postfix=png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jamA7Q4EzMNt"
   },
   "source": [
    "## Training Files\n",
    "\n",
    "There are 4 training data files located in the medmnist folder we will be using.\n",
    "\n",
    "1. [train_path_full.csv](medmnist/dataset/train_path_full.csv) with 90,000 images.\n",
    "2. [val_path_full.csv](medmnist/dataset/val_path_full.csv) with 10,000 validation images.\n",
    "3. [test_path_full](medmnist/dataset/test_path_full.csv) with 7200 images.\n",
    "\n",
    "**Note:** For this tutorial, we will be using the full PathMNIST dataset for training, validation and testing. However, to improve efficiency, you may consider using a fraction of this dataset instead with GaNDLF. If so, you can access the appropriate data CSV files at these links:\n",
    "\n",
    "*   `train_path_tiny` (4,000 images): https://app.box.com/index.php?rm=box_download_shared_file&shared_name=um4003lkrvyj55jm4a0jz7zsuokb0r8o&file_id=f_991821586980\n",
    "\n",
    "*   `val_path_tiny` (1,000 images): https://app.box.com/index.php?rm=box_download_shared_file&shared_name=rsmff27sm2z34r5xso1jx8xix7nhfspc&file_id=f_991817441206\n",
    "\n",
    "*   `test_path_tiny` (500 images): https://app.box.com/index.php?rm=box_download_shared_file&shared_name=22lm0qfzk5luap72mtdpzx5l3ocflopa&file_id=f_991819617152\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsV-6ZgMzbIf"
   },
   "source": [
    "### The Config File:\n",
    "\n",
    "For the purposes of this tutorial, we have already constructed a config file for this specific task, but for other tasks and experiments that you may want to run, this file will need to be edited to fit the required specifications of your experiment. However, the overall structure of this file will stay the same regardless of your task, and so you should be able to get by by simply downloading and editing the config.yaml file we're using below for use in your own experiments.\n",
    "\n",
    "Either way, we highly encourage you to take a look at the structure of this file before proceeding if you intend to use GaNDLF for your own experiments, as it will be the backbone of all tasks you use GaNDLF with in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3tG2RD-zxsl"
   },
   "source": [
    "#### Let's visualize some sample images and their classes from the PathMNIST dataset.\n",
    "\n",
    "#### Image classes for reference:\n",
    "**Class 0:** Adipose\n",
    "\n",
    "**Class 1:** Background\n",
    "\n",
    "**Class 2:** Debris\n",
    "\n",
    "**Class 3:** Lymphocytes\n",
    "\n",
    "**Class 4:** Mucus\n",
    "\n",
    "**Class 5:** Smooth Muscle\n",
    "\n",
    "**Class 6:** Normal Colon Mucosa\n",
    "\n",
    "**Class 7:** Cancer-Associated Stroma\n",
    "\n",
    "**Class 8:** Colorectal Adenocarcinoma Epithelium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "cYz9Kux_TAU5",
    "outputId": "eefbf4c2-2e3b-4160-fa43-c55915140f6a",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "\n",
    "df_pathmnist = pd.read_csv('./medmnist/dataset/pathmnist.csv')\n",
    "\n",
    "selected_images = [32, 36, 46, 13, 14, 8, 12, 18, 5, 6, 17, 0, 16, 3, 7, 10, 43, 45, 55, 1, 31, 41, 4, 9, 11]\n",
    "\n",
    "fig, ax = plt.subplot_mosaic([\n",
    "    ['img0', 'img1', 'img2', 'img3', 'img4'],\n",
    "    ['img5', 'img6', 'img7', 'img8', 'img9'],\n",
    "    ['img10', 'img11', 'img12', 'img13', 'img14'],\n",
    "    ['img15', 'img16', 'img17', 'img18', 'img19'],\n",
    "    ['img20', 'img21', 'img22', 'img23', 'img24']\n",
    "], figsize=(15, 15))\n",
    "\n",
    "for i in range(len(selected_images)):\n",
    "  img = selected_images[i]\n",
    "  filename = df_pathmnist.iloc[img]['train0_0.png']\n",
    "  img_class = df_pathmnist.iloc[img]['0']\n",
    "\n",
    "  path_img = mpimg.imread(f'./medmnist/dataset/pathmnist/{filename}')\n",
    "\n",
    "  ax[f'img{i}'].imshow(path_img)\n",
    "  ax[f'img{i}'].axis('off')\n",
    "  ax[f'img{i}'].set_title(f'Class {img_class}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUw5dzQ90MbB"
   },
   "source": [
    "#### Now, on to training! Since there is only one GPU, let's set `CUDA_VISIBLE_DEVICES` to 0 to train on the first (and only) available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHOw4AcP6CmQ",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsnH7GCS0TQd"
   },
   "source": [
    "#### Let's run the training script! For this run, we will pass in the [config.yaml](config.yaml) file that we just downloaded to the `-c` parameter, the training and validation CSV files to the `-i` parameter, and the model directory to the `-m` parameter (folder will automatically be created if it doesn't exist, which, in our case, it doesn't). We will also specify `-t True` to indicate that we are training and `-d cuda` to indicate that we will be training on the GPU. For demonstration purposes, we will only be training on 5 epochs.\n",
    "\n",
    "#### You will likely notice a persistent error saying \"y_pred contains classes not in y_true\"--this is a known issue and will not affect our training performance, so feel free to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpcV08u6X9sK",
    "outputId": "2927f3b1-a69d-4f68-c5a4-ca519f6d9b11",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!python ../gandlf_run -c config.yaml -i medmnist/dataset/train_path_full.csv,medmnist/dataset/val_path_full.csv -m model/ -t True -d cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ1SKFYZ0o0F"
   },
   "source": [
    "#### Now that training is complete, let's collect and save model statistics to the `output_stats` folder. Using `-c True` indicates that we'd like the 4 plots ordinarily generated by this command to be combined into two plots by overlaying training and validation statistics on the same graphs instead of keeping them separate. Feel free to experiment with this command by using `-c False` instead and viewing the resulting plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMFrdeM1G4bE",
    "outputId": "7db89cf2-12fa-43d1-e2e9-7c5fad846e3c",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!python gandlf_collectStats_final -m model/ -o output_stats -c True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaSLcs8p04tm"
   },
   "source": [
    "#### Now, let's view our generated plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KnduQsiQrS2v",
    "outputId": "53631df1-842f-47cf-ea83-04f3e6ba107c",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"tutorial/output_stats/plot.png\", width=1500, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUyXNnRWHKAM"
   },
   "source": [
    "#### Since we only trained the model on a very small number of epochs, we shouldn't be expecting very impressive results here. However, from the graphs, we can tell that accuracy is steadily increasing and loss is steadily decreasing, which is a great sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yoR6sCr0hzZ"
   },
   "source": [
    "#### Finally, let's run the inference script. This is almost identical to running the training script; however, note that the argument for the `-t` parameter has been changed from `True` to `False` to specify that we are not training, and we are using the `test_path_full` csv file to access the testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XL4qjJu0msz",
    "outputId": "4e5cc630-8988-4f62-fad8-f66076cfb23b",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!python ../gandlf_run -c config.yaml -i medmnist/dataset/test_path_full.csv -m model/ -t False -d cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etNTiliFykKL"
   },
   "source": [
    "#### Now that inference is complete, let's view some sample test images along with their predicted and ground truth classes to get a visual idea of how well our model did on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "qds3TbeNlgdO",
    "outputId": "3c928bd2-c313-4072-f3e4-29a45b942d13"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "\n",
    "df_preds = pd.read_csv('./model/final_preds_and_avg_probs.csv')\n",
    "\n",
    "selected_images = [4, 7, 14, 22, 26, 3, 11, 17, 37, 45, 52, 1, 2, 40, 47, 76, 8, 13, 28, 19, 36, 79, 0, 9, 10]\n",
    "\n",
    "fig, ax = plt.subplot_mosaic([\n",
    "    ['img0', 'img1', 'img2', 'img3', 'img4'],\n",
    "    ['img5', 'img6', 'img7', 'img8', 'img9'],\n",
    "    ['img10', 'img11', 'img12', 'img13', 'img14'],\n",
    "    ['img15', 'img16', 'img17', 'img18', 'img19'],\n",
    "    ['img20', 'img21', 'img22', 'img23', 'img24']\n",
    "], figsize=(13, 13), constrained_layout = True)\n",
    "\n",
    "for i in range(len(selected_images)):\n",
    "  img = selected_images[i]\n",
    "  filename = df_preds.iloc[img]['SubjectID']\n",
    "  ground_truth = filename.split('_')[1].split('.')[0]\n",
    "  pred_class = df_preds.iloc[img]['PredictedClass']\n",
    "\n",
    "  path_img = mpimg.imread(f'./medmnist/dataset/pathmnist/{filename}')\n",
    "\n",
    "  ax[f'img{i}'].imshow(path_img)\n",
    "  ax[f'img{i}'].axis('off')\n",
    "  ax[f'img{i}'].set_title(f'Predicted Class: {pred_class}\\nGround Truth: {ground_truth}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBkKQsylygDO"
   },
   "source": [
    "#### To conclude this tutorial, let's zoom out and take a look at how well our model did as a whole on each class by constructing a confusion matrix from our inference data.\n",
    "\n",
    "#### Note: if you'd like, feel free to change the colormap of the confusion matrix (denoted by \"cmap\" in the cm_display.plot() command) to your liking. Here's a list of some of the most popular colormaps: `viridis` (default), `plasma`, `inferno`, `magma`, `cividis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "ZNw9DVl5uX-B",
    "outputId": "f1a60de9-8c7b-415d-a3c3-a219d356954f"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "gt_list = []\n",
    "pred_list = []\n",
    "\n",
    "for i in range(len(df_preds)):\n",
    "  filename = df_preds.iloc[i]['SubjectID']\n",
    "\n",
    "  ground_truth = int(filename.split('_')[1].split('.')[0])\n",
    "  pred_class = int(df_preds.iloc[i]['PredictedClass'])\n",
    "\n",
    "  gt_list.append(ground_truth)\n",
    "  pred_list.append(pred_class)\n",
    "\n",
    "cm = confusion_matrix(gt_list, pred_list)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "cm_display.plot(cmap = 'viridis', ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DA9VczBEnH8"
   },
   "source": [
    "#### Here, we can see that while the model performed well overall, it had difficulties when it came to images of Class 7, incorrectly predicting a majority of them as belonging to Class 2 instead. We can see a similar trend with images of Class 5, with the model incorrectly predicting most of them as belonging to Class 2. \n",
    "\n",
    "#### Given the appearance of the accuracy and loss plots, had we trained on more epochs, we would have expected these results to improve. However, given that we only trained on 5 epochs, these are great results. Indeed, the model did very well on other classes, including Classes 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "74UyQOGn0Afx"
   },
   "source": [
    "### That concludes this GaNDLF tutorial! Hopefully, this tutorial was helpful to you in understanding how GaNDLF works as well as how to apply it to your own projects. If you need any additional information about GaNDLF's usage and capabilities, please consult the GitHub repo (https://mlcommons.github.io/GaNDLF) and the documentation (https://mlcommons.github.io/GaNDLF). For more questions and support, please visit the Discussions page on GitHub (https://github.com/mlcommons/GaNDLF/discussions)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
