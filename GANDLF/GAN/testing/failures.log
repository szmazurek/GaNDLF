device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:513: in training_loop_gans
    ) = train_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...nplace=True)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f14052cbe50>
optimizer_g = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
optimizer_d = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def train_network_gan(
        model, train_dataloader, optimizer_g, optimizer_d, params
    ):
        """
        Function to train a GAN network for a single epoch.
        This function is a modified version of train_network() to support
        usage of two optimizers for the generator and discriminator.
    
        Parameters
        ----------
        model : torch.model
            The model to process the input image with, it should support appropriate dimensions.
        train_dataloader : torch.DataLoader
            The dataloader for the training epoch
        optimizer_g : torch.optim
            Optimizer for optimizing generator network
        optimizer_d : torch.optim
            Optimizer for optimizing discriminator network
        params : dict
            the parameters passed by the user yaml
    
        Returns
        -------
        average_epoch_train_loss_gen : float
            Train loss for the current epoch for generator
        average_epoch_train_loss_disc : float
            Train loss for the current epoch for discriminator
        average_epoch_train_metric : dict
            Train metrics for the current epoch
        """
    
        print("*" * 20)
        print("Starting Training : ")
        print("*" * 20)
        # Initialize a few things
        total_epoch_train_loss_gen = 0
        total_epoch_train_loss_disc = 0
        total_epoch_train_metric = {}
        average_epoch_train_metric = {}
        for metric in params["metrics"]:
            if "per_label" in metric:  # conditional generation not yet implemented
                total_epoch_train_metric[metric] = []
            else:
                total_epoch_train_metric[metric] = 0
    
        # automatic mixed precision - https://pytorch.org/docs/stable/amp.html
        if params["model"]["amp"]:
            scaler = GradScaler()
            if params["verbose"]:
                print("Using Automatic mixed precision", flush=True)
    
        # Set the model to train
        model.train()
        for batch_idx, (subject) in enumerate(
            tqdm(train_dataloader, desc="Looping over training data")
        ):
            #### DISCRIMINATOR STEP WITH ALL REAL LABELS ####
            optimizer_d.zero_grad()
            image_real = (
                torch.cat(
                    [subject[key][torchio.DATA] for key in params["channel_keys"]],
                    dim=1,
                )
                .float()
                .to(params["device"])
            )
            current_batch_size = image_real.shape[0]
    
>           label_real = torch.full(
                current_batch_size,
                1,
                dtype=torch.float,
                device=params["device"],
            )
E           TypeError: full() received an invalid combination of arguments - got (int, int, device=torch.device, dtype=torch.dtype), but expected one of:
E            * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
E            * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:106: TypeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:511: in training_loop_gans
    ) = train_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...nplace=True)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f3397f9fe20>
optimizer_g = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
optimizer_d = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def train_network_gan(
        model, train_dataloader, optimizer_g, optimizer_d, params
    ):
        """
        Function to train a GAN network for a single epoch.
        This function is a modified version of train_network() to support
        usage of two optimizers for the generator and discriminator.
    
        Parameters
        ----------
        model : torch.model
            The model to process the input image with, it should support appropriate dimensions.
        train_dataloader : torch.DataLoader
            The dataloader for the training epoch
        optimizer_g : torch.optim
            Optimizer for optimizing generator network
        optimizer_d : torch.optim
            Optimizer for optimizing discriminator network
        params : dict
            the parameters passed by the user yaml
    
        Returns
        -------
        average_epoch_train_loss_gen : float
            Train loss for the current epoch for generator
        average_epoch_train_loss_disc : float
            Train loss for the current epoch for discriminator
        average_epoch_train_metric : dict
            Train metrics for the current epoch
        """
    
        print("*" * 20)
        print("Starting Training : ")
        print("*" * 20)
        # Initialize a few things
        total_epoch_train_loss_gen = 0
        total_epoch_train_loss_disc = 0
        total_epoch_train_metric = {}
        average_epoch_train_metric = {}
        for metric in params["metrics"]:
            if "per_label" in metric:  # conditional generation not yet implemented
                total_epoch_train_metric[metric] = []
            else:
                total_epoch_train_metric[metric] = 0
    
        # automatic mixed precision - https://pytorch.org/docs/stable/amp.html
        if params["model"]["amp"]:
            scaler = GradScaler()
            if params["verbose"]:
                print("Using Automatic mixed precision", flush=True)
    
        # Set the model to train
        model.train()
        for batch_idx, (subject) in enumerate(
            tqdm(train_dataloader, desc="Looping over training data")
        ):
            #### DISCRIMINATOR STEP WITH ALL REAL LABELS ####
            optimizer_d.zero_grad()
            image_real = (
                torch.cat(
                    [subject[key][torchio.DATA] for key in params["channel_keys"]],
                    dim=1,
                )
                .float()
                .to(params["device"])
            )
            current_batch_size = image_real.shape[0]
    
>           label_real = torch.full(
                size=current_batch_size,
                fill_value=1,
                dtype=torch.float,
                device=params["device"],
            )
E           TypeError: full() received an invalid combination of arguments - got (device=torch.device, dtype=torch.dtype, fill_value=int, size=int, ), but expected one of:
E            * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
E            * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:106: TypeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:512: in training_loop_gans
    ) = train_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...nplace=True)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7fa589ec3ca0>
optimizer_g = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
optimizer_d = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def train_network_gan(
        model, train_dataloader, optimizer_g, optimizer_d, params
    ):
        """
        Function to train a GAN network for a single epoch.
        This function is a modified version of train_network() to support
        usage of two optimizers for the generator and discriminator.
    
        Parameters
        ----------
        model : torch.model
            The model to process the input image with, it should support appropriate dimensions.
        train_dataloader : torch.DataLoader
            The dataloader for the training epoch
        optimizer_g : torch.optim
            Optimizer for optimizing generator network
        optimizer_d : torch.optim
            Optimizer for optimizing discriminator network
        params : dict
            the parameters passed by the user yaml
    
        Returns
        -------
        average_epoch_train_loss_gen : float
            Train loss for the current epoch for generator
        average_epoch_train_loss_disc : float
            Train loss for the current epoch for discriminator
        average_epoch_train_metric : dict
            Train metrics for the current epoch
        """
    
        print("*" * 20)
        print("Starting Training : ")
        print("*" * 20)
        # Initialize a few things
        total_epoch_train_loss_gen = 0
        total_epoch_train_loss_disc = 0
        total_epoch_train_metric = {}
        average_epoch_train_metric = {}
        for metric in params["metrics"]:
            if "per_label" in metric:  # conditional generation not yet implemented
                total_epoch_train_metric[metric] = []
            else:
                total_epoch_train_metric[metric] = 0
    
        # automatic mixed precision - https://pytorch.org/docs/stable/amp.html
        if params["model"]["amp"]:
            scaler = GradScaler()
            if params["verbose"]:
                print("Using Automatic mixed precision", flush=True)
    
        # Set the model to train
        model.train()
        for batch_idx, (subject) in enumerate(
            tqdm(train_dataloader, desc="Looping over training data")
        ):
            #### DISCRIMINATOR STEP WITH ALL REAL LABELS ####
            optimizer_d.zero_grad()
            image_real = (
                torch.cat(
                    [subject[key][torchio.DATA] for key in params["channel_keys"]],
                    dim=1,
                )
                .float()
                .to(params["device"])
            )
            current_batch_size = image_real.shape[0]
            print("Current batch size : ", current_batch_size)
            print("device : ", params["device"])
>           label_real = torch.full(
                size=current_batch_size,
                fill_value=1,
                dtype=torch.float,
                device=params["device"],
            )
E           TypeError: full() received an invalid combination of arguments - got (device=torch.device, dtype=torch.dtype, fill_value=int, size=int, ), but expected one of:
E            * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
E            * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:107: TypeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:512: in training_loop_gans
    ) = train_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...nplace=True)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f0a7c9bba90>
optimizer_g = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
optimizer_d = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def train_network_gan(
        model, train_dataloader, optimizer_g, optimizer_d, params
    ):
        """
        Function to train a GAN network for a single epoch.
        This function is a modified version of train_network() to support
        usage of two optimizers for the generator and discriminator.
    
        Parameters
        ----------
        model : torch.model
            The model to process the input image with, it should support appropriate dimensions.
        train_dataloader : torch.DataLoader
            The dataloader for the training epoch
        optimizer_g : torch.optim
            Optimizer for optimizing generator network
        optimizer_d : torch.optim
            Optimizer for optimizing discriminator network
        params : dict
            the parameters passed by the user yaml
    
        Returns
        -------
        average_epoch_train_loss_gen : float
            Train loss for the current epoch for generator
        average_epoch_train_loss_disc : float
            Train loss for the current epoch for discriminator
        average_epoch_train_metric : dict
            Train metrics for the current epoch
        """
    
        print("*" * 20)
        print("Starting Training : ")
        print("*" * 20)
        # Initialize a few things
        total_epoch_train_loss_gen = 0
        total_epoch_train_loss_disc = 0
        total_epoch_train_metric = {}
        average_epoch_train_metric = {}
        for metric in params["metrics"]:
            if "per_label" in metric:  # conditional generation not yet implemented
                total_epoch_train_metric[metric] = []
            else:
                total_epoch_train_metric[metric] = 0
    
        # automatic mixed precision - https://pytorch.org/docs/stable/amp.html
        if params["model"]["amp"]:
            scaler = GradScaler()
            if params["verbose"]:
                print("Using Automatic mixed precision", flush=True)
    
        # Set the model to train
        model.train()
        for batch_idx, (subject) in enumerate(
            tqdm(train_dataloader, desc="Looping over training data")
        ):
            #### DISCRIMINATOR STEP WITH ALL REAL LABELS ####
            optimizer_d.zero_grad()
            image_real = (
                torch.cat(
                    [subject[key][torchio.DATA] for key in params["channel_keys"]],
                    dim=1,
                )
                .float()
                .to(params["device"])
            )
            current_batch_size = image_real.shape[0]
            print("Current batch size : ", current_batch_size)
            print("device : ", params["device"])
>           label_real = torch.full(
                size=current_batch_size,
                fill_value=1,
                dtype=torch.float,
                device=torch.device(params["device"]),
            )
E           TypeError: full() received an invalid combination of arguments - got (device=torch.device, dtype=torch.dtype, fill_value=int, size=int, ), but expected one of:
E            * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
E            * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:107: TypeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:512: in training_loop_gans
    ) = train_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...nplace=True)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f06be1bfa90>
optimizer_g = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
optimizer_d = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def train_network_gan(
        model, train_dataloader, optimizer_g, optimizer_d, params
    ):
        """
        Function to train a GAN network for a single epoch.
        This function is a modified version of train_network() to support
        usage of two optimizers for the generator and discriminator.
    
        Parameters
        ----------
        model : torch.model
            The model to process the input image with, it should support appropriate dimensions.
        train_dataloader : torch.DataLoader
            The dataloader for the training epoch
        optimizer_g : torch.optim
            Optimizer for optimizing generator network
        optimizer_d : torch.optim
            Optimizer for optimizing discriminator network
        params : dict
            the parameters passed by the user yaml
    
        Returns
        -------
        average_epoch_train_loss_gen : float
            Train loss for the current epoch for generator
        average_epoch_train_loss_disc : float
            Train loss for the current epoch for discriminator
        average_epoch_train_metric : dict
            Train metrics for the current epoch
        """
    
        print("*" * 20)
        print("Starting Training : ")
        print("*" * 20)
        # Initialize a few things
        total_epoch_train_loss_gen = 0
        total_epoch_train_loss_disc = 0
        total_epoch_train_metric = {}
        average_epoch_train_metric = {}
        for metric in params["metrics"]:
            if "per_label" in metric:  # conditional generation not yet implemented
                total_epoch_train_metric[metric] = []
            else:
                total_epoch_train_metric[metric] = 0
    
        # automatic mixed precision - https://pytorch.org/docs/stable/amp.html
        if params["model"]["amp"]:
            scaler = GradScaler()
            if params["verbose"]:
                print("Using Automatic mixed precision", flush=True)
    
        # Set the model to train
        model.train()
        for batch_idx, (subject) in enumerate(
            tqdm(train_dataloader, desc="Looping over training data")
        ):
            #### DISCRIMINATOR STEP WITH ALL REAL LABELS ####
            optimizer_d.zero_grad()
            image_real = (
                torch.cat(
                    [subject[key][torchio.DATA] for key in params["channel_keys"]],
                    dim=1,
                )
                .float()
                .to(params["device"])
            )
            current_batch_size = image_real.shape[0]
            print("Current batch size : ", current_batch_size)
            print("device : ", params["device"])
>           label_real = torch.full(
                size=(current_batch_size),
                fill_value=1,
                dtype=torch.float,
                device=torch.device(params["device"]),
            )
E           TypeError: full() received an invalid combination of arguments - got (device=torch.device, dtype=torch.dtype, fill_value=int, size=int, ), but expected one of:
E            * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
E            * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:107: TypeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:512: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:124: in train_network_gan
    loss_disc_real, _, output_disc_real, _ = step_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/step.py:89: in step_gan
    loss, metric_output = get_loss_and_metrics_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/loss_and_metric.py:80: in get_loss_and_metrics_gans
    loss = get_loss_gans(predictions, labels, params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

predictions = tensor([[0.4916]], grad_fn=<SigmoidBackward0>), labels = tensor([1.]), params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def get_loss_gans(predictions: Tensor, labels: Tensor, params: dict) -> Tensor:
        """
        Compute the loss value for adversatial generative networks.
        Args:
            predictions (Tensor): The predicted output from the model.
            labels (Tensor): The ground truth label.
            params (dict): The parameters passed by the user yaml.
        Returns:
            loss (Tensor): The computed loss from the label and the prediction.
        """
    
        if isinstance(params["loss_function"], dict):
            # check for mse_torch
            loss_function = global_losses_dict[
                list(params["loss_function"].keys())[0]
            ]
        else:
            loss_str_lower = params["loss_function"].lower()
            if loss_str_lower in global_losses_dict:
                loss_function = global_losses_dict[loss_str_lower]
            else:
                sys.exit(
                    "WARNING: Could not find the requested loss function '"
                    + params["loss_function"]
                )
    
>       loss = loss_function(predictions, labels, params)
E       TypeError: CE() takes 2 positional arguments but 3 were given

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/loss_and_metric.py:53: TypeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:511: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:123: in train_network_gan
    loss_disc_real, _, output_disc_real, _ = step_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/step.py:89: in step_gan
    loss, metric_output = get_loss_and_metrics_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/loss_and_metric.py:80: in get_loss_and_metrics_gans
    loss = get_loss_gans(predictions, labels, params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

predictions = tensor([[0.5843]], grad_fn=<SigmoidBackward0>), labels = tensor([1.]), params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def get_loss_gans(predictions: Tensor, labels: Tensor, params: dict) -> Tensor:
        """
        Compute the loss value for adversatial generative networks.
        Args:
            predictions (Tensor): The predicted output from the model.
            labels (Tensor): The ground truth label.
            params (dict): The parameters passed by the user yaml.
        Returns:
            loss (Tensor): The computed loss from the label and the prediction.
        """
    
        if isinstance(params["loss_function"], dict):
            # check for mse_torch
            loss_function = global_losses_dict[
                list(params["loss_function"].keys())[0]
            ]
        else:
            loss_str_lower = params["loss_function"].lower()
            if loss_str_lower in global_losses_dict:
                loss_function = global_losses_dict[loss_str_lower]
            else:
                sys.exit(
                    "WARNING: Could not find the requested loss function '"
                    + params["loss_function"]
                )
    
>       loss = loss_function(predictions, labels, params)
E       TypeError: CE() takes 2 positional arguments but 3 were given

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/loss_and_metric.py:53: TypeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:511: in training_loop_gans
    ) = train_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...nplace=True)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
train_dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f15919af970>
optimizer_g = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
optimizer_d = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
  ...-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 5e-05
)
params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}

    def train_network_gan(
        model, train_dataloader, optimizer_g, optimizer_d, params
    ):
        """
        Function to train a GAN network for a single epoch.
        This function is a modified version of train_network() to support
        usage of two optimizers for the generator and discriminator.
    
        Parameters
        ----------
        model : torch.model
            The model to process the input image with, it should support appropriate dimensions.
        train_dataloader : torch.DataLoader
            The dataloader for the training epoch
        optimizer_g : torch.optim
            Optimizer for optimizing generator network
        optimizer_d : torch.optim
            Optimizer for optimizing discriminator network
        params : dict
            the parameters passed by the user yaml
    
        Returns
        -------
        average_epoch_train_loss_gen : float
            Train loss for the current epoch for generator
        average_epoch_train_loss_disc : float
            Train loss for the current epoch for discriminator
        average_epoch_train_metric : dict
            Train metrics for the current epoch
        """
    
        print("*" * 20)
        print("Starting Training : ")
        print("*" * 20)
        # Initialize a few things
        total_epoch_train_loss_gen = 0
        total_epoch_train_loss_disc = 0
        total_epoch_train_metric = {}
        average_epoch_train_metric = {}
        for metric in params["metrics"]:
            if "per_label" in metric:  # conditional generation not yet implemented
                total_epoch_train_metric[metric] = []
            else:
                total_epoch_train_metric[metric] = 0
    
        # automatic mixed precision - https://pytorch.org/docs/stable/amp.html
        if params["model"]["amp"]:
            scaler = GradScaler()
            if params["verbose"]:
                print("Using Automatic mixed precision", flush=True)
    
        # Set the model to train
        model.train()
        for batch_idx, (subject) in enumerate(
            tqdm(train_dataloader, desc="Looping over training data")
        ):
            #### DISCRIMINATOR STEP WITH ALL REAL LABELS ####
            optimizer_d.zero_grad()
            image_real = (
                torch.cat(
                    [subject[key][torchio.DATA] for key in params["channel_keys"]],
                    dim=1,
                )
                .float()
                .to(params["device"])
            )
            current_batch_size = image_real.shape[0]
    
            label_real = torch.full(
                size=(current_batch_size,),
                fill_value=1,
                dtype=torch.float,
                device=params["device"],
            )
    
            if params["save_training"]:
                write_training_patches(
                    subject,
                    params,
                )
            # ensure spacing is always present in params and is always subject-specific
            if "spacing" in subject:
                params["subject_spacing"] = subject["spacing"]
            else:
                params["subject_spacing"] = None
            loss_disc_real, _, output_disc_real, _ = step_gan(
                model, image_real, label_real, params, secondary_images=None
            )
            nan_loss = torch.isnan(loss_disc_real)
            second_order = (
                hasattr(optimizer_d, "is_second_order")
                and optimizer_d.is_second_order
            )
            if params["model"]["amp"]:
                with torch.cuda.amp.autocast():
                    # if loss is nan, don't backprop and don't step optimizer
                    if not nan_loss:
                        scaler(
                            loss=loss_disc_real,
                            optimizer=optimizer_d,
                            clip_grad=params["clip_grad"],
                            clip_mode=params["clip_mode"],
                            parameters=model_parameters_exclude_head(
                                model, clip_mode=params["clip_mode"]
                            ),
                            create_graph=second_order,
                        )
            else:
                if not nan_loss:
                    loss_disc_real.backward(create_graph=second_order)
                    if params["clip_grad"] is not None:
                        dispatch_clip_grad_(
                            parameters=model_parameters_exclude_head(
                                model, clip_mode=params["clip_mode"]
                            ),
                            value=params["clip_grad"],
                            mode=params["clip_mode"],
                        )
    
            #### DISCRIMINATOR STEP WITH ALL FAKE LABELS ####
            latent_vector = torch.randn(
                current_batch_size,
>               params["model"]["latent_vector_size"],
                device=params["device"],
            )
E           KeyError: 'latent_vector_size'

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:160: KeyError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:511: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:164: in train_network_gan
    fake_images = model.generator(latent_vector)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/models/dcgan.py:178: in forward
    out = self.feature_extractor(x)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/container.py:215: in forward
    input = module(input)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = ConvTranspose2d(100, 4, kernel_size=(4, 4), stride=(1, 1), bias=False)
input = tensor([[ 0.6329,  1.6758,  0.7465,  0.2742,  0.3970,  0.7624,  1.4102, -0.4160,
          2.5015,  1.1939,  1.2833, -... -0.5962,  0.2316, -0.7123,  0.3340, -1.2913,  1.1229, -0.4159, -1.2324,
         -0.5949,  1.1005, -0.1164,  0.9811]])
output_size = None

    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
        if self.padding_mode != 'zeros':
            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
    
        assert isinstance(self.padding, tuple)
        # One cannot replace List by Tuple or Sequence in "_output_padding" because
        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
        num_spatial_dims = 2
        output_padding = self._output_padding(
            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
            num_spatial_dims, self.dilation)  # type: ignore[arg-type]
    
>       return F.conv_transpose2d(
            input, self.weight, self.bias, self.stride, self.padding,
            output_padding, self.groups, self.dilation)
E       RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1, 100]

../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:511: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:164: in train_network_gan
    fake_images = model.generator(latent_vector)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/models/dcgan.py:178: in forward
    out = self.feature_extractor(x)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/container.py:215: in forward
    input = module(input)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = ConvTranspose2d(100, 4, kernel_size=(4, 4), stride=(1, 1), bias=False)
input = tensor([[-1.1561,  0.9187,  0.9107,  0.2986, -1.5107, -0.5398,  0.1292,  1.5422,
         -0.6223,  1.3872,  1.0089, -...  0.2546, -0.8392, -0.2045,  0.4308, -1.1088, -0.1921,  1.4699,  0.1894,
          0.4639, -0.7800,  0.2625,  1.3841]])
output_size = None

    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
        if self.padding_mode != 'zeros':
            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
    
        assert isinstance(self.padding, tuple)
        # One cannot replace List by Tuple or Sequence in "_output_padding" because
        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
        num_spatial_dims = 2
        output_padding = self._output_padding(
            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
            num_spatial_dims, self.dilation)  # type: ignore[arg-type]
    
>       return F.conv_transpose2d(
            input, self.weight, self.bias, self.stride, self.padding,
            output_padding, self.groups, self.dilation)
E       RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1, 100]

../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:362: in training_loop_gans
    ) = create_pytorch_objects_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/generic.py:75: in create_pytorch_objects_gan
    model = get_model(parameters)
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/models/__init__.py:135: in get_model
    return global_models_dict[params["model"]["architecture"]](
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = DCGAN(), parameters = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'clip_grad': None, ...}

    def __init__(self, parameters: Dict):
        ModelBase.__init__(self, parameters)
        if not ("latent_vector_size" in parameters["model"]):
            warn(
                "No latent vector dimension specified. Defaulting to 100.",
                RuntimeWarning,
            )
            parameters["latent_vector_size"] = 100
        if not ("growth_rate" in parameters):
            parameters["growth_rate"] = 2
        if not ("bn_size" in parameters):
            parameters["bn_size"] = 4
        if not ("slope" in parameters):
            parameters["slope"] = 0.2
        if not ("drop_rate" in parameters):
            parameters["drop_rate"] = 0.0
        if not ("conv1_t_stride" in parameters):
            parameters["conv1_t_stride"] = 1
        if not ("conv1_t_size" in parameters):
            parameters["conv1_t_size"] = 7
        if self.Norm is None:
            warn(
                "No normalization specified. Defaulting to BatchNorm",
                RuntimeWarning,
            )
            self.Norm = self.BatchNorm
        self.generator = _GneratorDCGAN(
            self.patch_size,
            self.n_dimensions,
>           parameters["latent_vector_size"],
            self.n_channels,
            parameters["growth_rate"],
            parameters["bn_size"],
            parameters["slope"],
            self.Norm,
            self.ConvTranspose,
        )
E       KeyError: 'latent_vector_size'

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/models/dcgan.py:353: KeyError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:511: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:164: in train_network_gan
    fake_images = model.generator(latent_vector)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/models/dcgan.py:178: in forward
    out = self.feature_extractor(x)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/container.py:215: in forward
    input = module(input)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = ConvTranspose2d(100, 4, kernel_size=(4, 4), stride=(1, 1), bias=False)
input = tensor([[-1.5358,  1.5527, -0.8963,  0.3540, -0.0979,  1.3040, -0.7189, -1.2939,
         -0.7807, -3.0211,  0.2213, -...  1.1350, -0.1680, -0.1506,  0.1224, -1.1811,  0.4956,  0.8389,  1.4172,
         -1.4004, -0.6184,  0.2775,  1.9061]])
output_size = None

    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
        if self.padding_mode != 'zeros':
            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
    
        assert isinstance(self.padding, tuple)
        # One cannot replace List by Tuple or Sequence in "_output_padding" because
        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
        num_spatial_dims = 2
        output_padding = self._output_padding(
            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
            num_spatial_dims, self.dilation)  # type: ignore[arg-type]
    
>       return F.conv_transpose2d(
            input, self.weight, self.bias, self.stride, self.padding,
            output_padding, self.groups, self.dilation)
E       RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1, 100]

../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:513: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:166: in train_network_gan
    fake_images = model.generator(latent_vector)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/models/dcgan.py:178: in forward
    out = self.feature_extractor(x)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/container.py:215: in forward
    input = module(input)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = ConvTranspose2d(100, 4, kernel_size=(4, 4), stride=(1, 1), bias=False)
input = tensor([[ 1.2022,  0.3334, -0.7902, -1.2678,  0.0749,  1.0565,  0.8057,  1.1378,
         -0.3941,  0.5495,  0.8545, -... -0.5625, -0.9877, -1.2356,  1.2640,  0.4511,  1.3240, -0.4667, -2.7951,
          0.5826, -1.6159, -0.5016,  0.8954]])
output_size = None

    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
        if self.padding_mode != 'zeros':
            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
    
        assert isinstance(self.padding, tuple)
        # One cannot replace List by Tuple or Sequence in "_output_padding" because
        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
        num_spatial_dims = 2
        output_padding = self._output_padding(
            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
            num_spatial_dims, self.dilation)  # type: ignore[arg-type]
    
>       return F.conv_transpose2d(
            input, self.weight, self.bias, self.stride, self.padding,
            output_padding, self.groups, self.dilation)
E       RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1, 100]

../venvs/gandalf_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:537: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:254: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

tensors = (tensor(0.7625, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:510: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:189: in train_network_gan
    loss_disc.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.6806, grad_fn=<DivBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:510: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:189: in train_network_gan
    loss_disc.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.7531, grad_fn=<DivBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:512: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:191: in train_network_gan
    loss_disc.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.7770, grad_fn=<DivBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:536: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:253: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.8764, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:536: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:253: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.7188, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:537: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:254: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.7496, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:538: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:255: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.8727, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:536: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:253: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.7782, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:540: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:257: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.0620, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = False
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:540: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:257: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.0627, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:544: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:259: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.8515, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:545: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:260: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.0553, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:545: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:260: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.0935, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:546: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:261: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.1147, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:546: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:261: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.0112, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:546: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:261: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.1051, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:546: in training_loop_gans
    ) = train_network_gan(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:261: in train_network_gan
    loss_gen.backward(create_graph=second_order)
../venvs/gandalf_env/lib/python3.10/site-packages/torch/_tensor.py:492: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(0.8436, grad_fn=<BinaryCrossEntropyBackward0>),), grad_tensors = None, retain_graph = False, create_graph = False, grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms."
            )
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError(
                    "'grad_tensors' and 'grad_variables' (deprecated) "
                    "arguments both passed to backward(). Please only "
                    "use 'grad_tensors'."
                )
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (
            (inputs,)
            if isinstance(inputs, torch.Tensor)
            else tuple(inputs)
            if inputs is not None
            else tuple()
        )
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat the same comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors,
            grad_tensors_,
            retain_graph,
            create_graph,
            inputs,
            allow_unreachable=True,
            accumulate_grad=True,
        )  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

../venvs/gandalf_env/lib/python3.10/site-packages/torch/autograd/__init__.py:251: RuntimeError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:557: in training_loop_gans
    ) = validate_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...e_slope=0.2)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7fdee10cbbe0>, scheduler_d = <torch.optim.lr_scheduler.LambdaLR object at 0x7fdee11089a0>
scheduler_g = <torch.optim.lr_scheduler.LambdaLR object at 0x7fdee1108d30>, params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}, epoch = 0, mode = 'validation'

    def validate_network_gan(
        model: ModelBase,
        dataloader: torch.utils.data.DataLoader,
        scheduler_d: torch.optim.lr_scheduler,
        scheduler_g: torch.optim.lr_scheduler,
        params: dict,
        epoch: int = 0,
        mode: str = "validation",
    ) -> Tuple[float, float, dict]:
        """
        Function to validate the network for a single epoch for GANs.
    
        Parameters
        ----------
            model (ModelBase): The model to validate. If params["model"]["type"]
        is torch, then this is a torch.nn.Module wrapped in a ModelBase class.
        Otherwise this is OV exec_net.
            dataloader (torch.utils.data.DataLoader): The dataloader to use.
            scheduler_d (torch.optim.lr_scheduler): The scheduler for the discriminator.
            scheduler_g (torch.optim.lr_scheduler): The scheduler for the generator.
            params (dict): The parameters for the run.
            epoch (int): The epoch number.
            mode (str): The mode of operation, either 'validation' or 'inference'.
        used to write outputs if requested.
        Returns
        ----------
            average_epoch_generator_loss (float): The average loss for the generator.
            average_epoch_discriminator_loss (float): The average loss for the discriminator.
            average_epoch_metrics (dict): The average metrics for the epoch.
        """
        assert mode in [
            "validation",
            "inference",
        ], "Mode should be 'validation' or 'inference' "
    
        print("*" * 20)
        print("Starting " + mode + " : ")
        print("*" * 20)
        total_epoch_discriminator_fake_loss = 0.0
        total_epoch_discriminator_real_loss = 0.0
        total_epoch_metrics = {}
        for metric in params["metrics"]:
            total_epoch_metrics[metric] = 0.0
        subject_id_list = []
        is_inference = mode == "inference"
        if params["verbose"]:
            if params["model"]["amp"]:
                print("Using Automatic mixed precision", flush=True)
    
>       pathlib.Path(current_output_dir).mkdir(parents=True, exist_ok=True)
E       UnboundLocalError: local variable 'current_output_dir' referenced before assignment

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/forward_pass.py:103: UnboundLocalError
device = 'cpu'

    def test_train_segmentation_rad_2d(device):
        print("03: Starting 2D Rad segmentation tests")
        # read and parse csv
        parameters = parseConfigGAN(
            testingDir + "/config_generation.yaml", version_check_flag=False
        )
        training_data, parameters["headers"] = parseTrainingCSV(
            inputDir + "/train_2d_rad_segmentation.csv"
        )
        parameters["modality"] = "rad"
        parameters["patch_size"] = patch_size["2D"]
        parameters["model"]["dimension"] = 2
        parameters["model"]["class_list"] = [0, 255]
        parameters["model"]["amp"] = True
        parameters["model"]["num_channels"] = 3
        parameters["model"]["onnx_export"] = False
        parameters["model"]["print_summary"] = False
        parameters["data_preprocessing"]["resize_image"] = [224, 224]
        parameters = populate_header_in_parameters(
            parameters, parameters["headers"]
        )
        # read and initialize parameters for specific data dimension
        for model in all_models_generation:
            parameters["model"]["architecture"] = model
            parameters["nested_training"]["testing"] = -5
            parameters["nested_training"]["validation"] = -5
            sanitize_outputDir()
>           TrainingManagerGAN(
                dataframe=training_data,
                outputDir=outputDir,
                parameters=parameters,
                device=device,
                resume=False,
                reset=True,
            )

GANDLF/GAN/testing/test_gan.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/training_manager_gan.py:270: in TrainingManagerGAN
    training_loop_gans(
../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/training_loop.py:554: in training_loop_gans
    ) = validate_network_gan(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = DCGAN(
  (generator): _GneratorDCGAN(
    (feature_extractor): Sequential(
      (conv1t): ConvTranspose2d(100, 4, ker...e_slope=0.2)
      (linear2): Linear(in_features=128, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7f5742bbbd00>, scheduler_d = <torch.optim.lr_scheduler.LambdaLR object at 0x7f57429fc8e0>
scheduler_g = <torch.optim.lr_scheduler.LambdaLR object at 0x7f57429fcc70>, params = {'batch_size': 1, 'bn_size': 4, 'channel_keys': ['1'], 'class_weights': None, ...}, epoch = 0, mode = 'validation'

    def validate_network_gan(
        model: ModelBase,
        dataloader: torch.utils.data.DataLoader,
        scheduler_d: torch.optim.lr_scheduler,
        scheduler_g: torch.optim.lr_scheduler,
        params: dict,
        epoch: int = 0,
        mode: str = "validation",
    ) -> Tuple[float, float, dict]:
        """
        Function to validate the network for a single epoch for GANs.
    
        Parameters
        ----------
            model (ModelBase): The model to validate. If params["model"]["type"]
        is torch, then this is a torch.nn.Module wrapped in a ModelBase class.
        Otherwise this is OV exec_net.
            dataloader (torch.utils.data.DataLoader): The dataloader to use.
            scheduler_d (torch.optim.lr_scheduler): The scheduler for the discriminator.
            scheduler_g (torch.optim.lr_scheduler): The scheduler for the generator.
            params (dict): The parameters for the run.
            epoch (int): The epoch number.
            mode (str): The mode of operation, either 'validation' or 'inference'.
        used to write outputs if requested.
        Returns
        ----------
            average_epoch_generator_loss (float): The average loss for the generator.
            average_epoch_discriminator_loss (float): The average loss for the discriminator.
            average_epoch_metrics (dict): The average metrics for the epoch.
        """
        assert mode in [
            "validation",
            "inference",
        ], "Mode should be 'validation' or 'inference' "
    
        print("*" * 20)
        print("Starting " + mode + " : ")
        print("*" * 20)
        total_epoch_discriminator_fake_loss = 0.0
        total_epoch_discriminator_real_loss = 0.0
        total_epoch_metrics = {}
        for metric in params["metrics"]:
            total_epoch_metrics[metric] = 0.0
        subject_id_list = []
        is_inference = mode == "inference"
        if params["verbose"]:
            if params["model"]["amp"]:
                print("Using Automatic mixed precision", flush=True)
    
>       pathlib.Path(current_output_dir).mkdir(parents=True, exist_ok=True)
E       UnboundLocalError: local variable 'current_output_dir' referenced before assignment

../venvs/gandalf_env/lib/python3.10/site-packages/GANDLF/GAN/compute/forward_pass.py:103: UnboundLocalError
